---
title: "P8105_hw3_xz2809"
author: "Coco"
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(ggplot2)
library(patchwork)

theme_set(theme_bw())
```

##Problem 1
This problem uses the BRFSS data. here is the **code chunks** for data cleaning and formatting. 
```{r}
data("brfss_smart2010")
brfss_smart2010=
  janitor::clean_names(brfss_smart2010) %>% 
  filter(topic=="Overall Health") %>% 
  select(-topic) %>% 
  select(-class) %>% 
  select(-question) %>% 
  select(-sample_size) %>%
  select(-(confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(states = locationabbr)
brfss_smart2010 <- brfss_smart2010[, c("year", "states","locationdesc","excellent","very_good","good","fair","poor")]
```

Here is the **code chunk** to do the analysis of BRFSS data. 
```{r message = FALSE, warning = FALSE}
num_state= brfss_smart2010 %>%
  filter(year == 2002) %>% 
  count(states) 

num_state %>% 
  filter(n == 7)
```
The CT (Connecticut), FL(Florida) and NC(North Carolina) were observed at 7 locations. There are in total 49 observations, which is 49 states in US. 


Here is the **code chunk** to make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{r}
brfss_smart2010 %>% 
  group_by(year,states) %>% 
  count(states) %>% 
  ggplot(aes(x = year, y = n, color = states))+geom_line()+
  theme(legend.position = "bottom") 
```
The other 48 states has the similar distribution of number of locatinos observered. However, the Florida has over 40 locations observed in 2007 and year 2010, which looks like two outliers.

Here is the **code chunk** to make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
brfss_smart2010 %>% 
  filter(year==2002 | year==2006 | year==2010) %>% 
  filter(states =="NY") %>% 
  group_by(year) %>% 
  mutate (mean = mean(excellent),
          sd = sd(excellent)) %>% 
  distinct(year,locationdesc,mean,sd) %>% 
  select(year,mean,sd) %>%
  distinct(year,mean,sd) %>% 
  knitr::kable(digits = 3)
```
All of the means and standard deviations of the three years look similar with each, which means there is no major change over the 8 years. 

Here is the **code chunk** to make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time. 

```{r}
mean_five_cat = brfss_smart2010 %>% 
  group_by(year,states) %>% 
  mutate(
    Excellent_Mean = mean(excellent),
    Verygood_Mean = mean(very_good),
    Good_Mean = mean(good),
    Fair_Mean = mean(fair),
    Poor_Mean = mean(poor)) %>% 
  select(year,states,Excellent_Mean:Poor_Mean) %>% 
  group_by(year) %>% 
  distinct(states,Excellent_Mean,Verygood_Mean,Good_Mean,Fair_Mean,Poor_Mean) %>% 
  gather(key = cat, value = proportion, Excellent_Mean:Poor_Mean)
  
  
mean_five_cat$cat = as.factor(mean_five_cat$cat) 
mean_five_cat$cat = factor(mean_five_cat$cat,levels = c("Excellent_Mean","Verygood_Mean","Good_Mean","Fair_Mean","Poor_Mean"))

mean_five_cat %>% 
  ggplot(aes(x = year, y = proportion)) +
  geom_point(alpha = .5) + 
  facet_grid(~cat) + 
  viridis::scale_fill_viridis(discrete = TRUE)+
  theme(legend.position = "none")+
  scale_x_continuous(breaks = c(2002, 2006, 2010), 
                     labels = c("02", "06", "10"))
```
From the plot above, we observed that the proportino of each category are similar over 2002 - 2010. The "vary good" response is the most frequent category while "poor" response is the least frequent category. 

##Problem 2
This problem uses the Instacart data.
```{r}
data("instacart")
```

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
num_aisle = instacart %>% 
  count(aisle) %>% 
  filter(n == max(n))
```
There are in total 134 aisles and "Fresh Vegetable" is the most items ordered from. 

Here is the **code chunk** to make a plot that shows the number of items ordered in each aisle. 

```{r}
x = instacart %>% 
  group_by(aisle_id) %>% 
  count(product_id) %>% 
  group_by(aisle_id) %>% 
  mutate(num_items = sum(n)) %>% 
  distinct(aisle_id,num_items)
ggplot(x, aes(x = aisle_id, y = num_items)) + geom_point()
```
From the data above, we could observe that the numbers of items ordered from most aisles are under 25,000. There are only two of items have over 150,000 items ordered from, one of them is the Fresh Vegetable aisle, which is the maximum ordered number. This makes sense because people tend to shop the most frequent.

Here is the **code chunk** to make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits":

```{r}
popular_item = instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle=="packaged vegetables fruits") %>% 
  mutate(aisle = as.factor(aisle)) %>% 
  group_by(aisle,product_id,product_name) %>% 
  count() %>% 
  group_by(aisle) %>% 
  filter(n == max(n))

popular_item %>%  
  knitr::kable(digits = 1)
```
Light Brown Sugar is the most popular item in baking ingredients, ordered 499 times, Snack Sticks Chicken & Rice Recipe Dog Treats is the most popular item in dog food care, ordered 30 times, Organic Baby Spinach is the most popular item in packaged vegetables fruits, ordered 9784 times.

Here is the **code chunk** to make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:

```{r}
z = instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(order_hour_of_day,product_name,order_dow) %>% 
  group_by(product_name,order_dow) %>% 
  mutate(mean_hour_day = mean (order_hour_of_day)) %>% 
  distinct(product_name,mean_hour_day,order_dow) %>% 
  spread(key = order_dow, value = mean_hour_day) 

z %>% 
  knitr::kable(digits = 2)
```


##Problem3 
```{r}
data("ny_noaa")
```

```{r}
ny_noaa_df= ny_noaa %>% 
  separate(date, into = c("year","month","date"),sep = "-") %>% 
  janitor::clean_names() %>% 
  mutate(tmin = as.numeric(tmin),
         tmax = as.numeric(tmax),
         id = as.factor(id))
```

Here is the **code chunk** to calcualte for snow fall the most commonly observed values. 

```{r}
MaxTable <- function(x){
     m <- unique(x)
     m[which.max(tabulate(match(x,m)))]
}
MaxTable(ny_noaa_df$snow)

```
The most commonly observed values for snow fall is 0, which make sense because the most of the year, new york does not snwo.

Here is the **code chunk** to make a two-panel plot showing the average temperature in January and in July in each station across years.

```{r message = FALSE, warning = FALSE}

mean_ave_df = ny_noaa_df %>% 
  mutate(year = as.numeric(year),
         month = as.factor(month)) %>% 
  filter(month=="07" | month == "01") %>% 
  group_by(year,month) %>% 
  filter(!is.na(tmax) & !is.na(tmin)) %>% 
  mutate(ave_max = mean(tmax)) %>% 
  distinct(year, month, ave_max) 
  
plot_names <- c("01" = "Januarary", "07" = "July")

mean_ave_df%>%
  ggplot(aes(x = year, y = ave_max)) + 
  geom_point(alpha = 0.5) +
  facet_grid(~month, labeller = as_labeller(plot_names))+
  viridis::scale_fill_viridis(discrete = TRUE)+
  theme(legend.position = "none")+
  scale_x_continuous(breaks = c(1981, 1990, 2000, 2010), 
                     labels = c("1981", "1990","2000","2010"))

```
There is no outlier of mean temperature in June and July. The median mean temperature of Januarary lies around 0 degrees while the median mean temperature of July lies around 25 degress. 

Here is **code chunk** to make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r}
mean_ave_df%>%
  ggplot(aes(y = ave_max)) + 
  geom_boxplot(alpha = 0.5) +
  facet_grid(~month, labeller = as_labeller(plot_names))+
  viridis::scale_fill_viridis(discrete = TRUE)+
  theme(legend.position = "none")

d <- ggplot(ny_noaa_df, aes(tmax,tmin))
a =  d + geom_hex(bins = 15) + 
  theme(legend.position = "right")+
  ggtitle("Tmax vs Tmin")+
  labs(
    caption = "For the whole data set"
  )

b = ny_noaa_df %>% 
  select(year,snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, fill = year)) + 
  geom_density(alpha = 0.1)+
  theme(legend.position = "bottom",
        plot.margin = margin(2,.8,10,.8, "cm"))

a+b
```







